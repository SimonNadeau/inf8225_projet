{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbPPbjGdgQL7"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkitXUB3czP6"
   },
   "source": [
    "Word2Vec Skip-gram a été développé par Google. La documentation et les articles sont accessibles à l'adresse suivante :\n",
    "\n",
    "\n",
    "*   https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "\n",
    "La librairie Gensim de Python permet d'accéder à des plongements lexicaux pré-entraînés ainsi que d'entraîner des modèles. La documentation est accessible à l'adresse suivante :\n",
    "\n",
    "\n",
    "*   https://radimrehurek.com/gensim/index.html\n",
    "\n",
    "\n",
    "Certains éléments du code implémenté dans le cadre de ce projet sont inspirés d'éléments des tutoriels que l'on peut retrouver aux adresses suivantes :\n",
    "\n",
    "\n",
    "*   https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72\n",
    "*   https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28KyUDeTDotL"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgsxUcBQ-L4t"
   },
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOC2629OgH-r"
   },
   "source": [
    "Téléchargement des corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "ySgqgbi9wmT4",
    "outputId": "cfc52bb7-8a92-4e40-dfa5-7f1f04decab5"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VyWSeXNSgPSO"
   },
   "source": [
    "Génération de l'ensemble de données à partir des 10 premiers documents du corpus Brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVM4EN8Syagg"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "fileIDs = brown.fileids()[:10]\n",
    "text = brown.sents(fileIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "ROVgUcsww98A",
    "outputId": "f8c14ad0-8900-45bd-b810-780b65d9c174"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def sentsToWords(sents):\n",
    "  return [word for sent in sents for word in sent]\n",
    "\n",
    "words = sentsToWords(text)\n",
    "frequencies = Counter(words)\n",
    "\n",
    "print('Avant le prétraitrement et le retrait des mots rares :\\n')\n",
    "print(f'Nombre de mots : {len(words)}')\n",
    "print(f'Taille du vocabulaire : {len(frequencies)}')\n",
    "print(f\"Première phrase : {' '.join(text[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLrvOW27goRF"
   },
   "source": [
    "Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P-DIF4NRYaYK",
    "outputId": "22b78e2a-420b-4df3-c46c-48fa1ac215a5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from time import time\n",
    "\n",
    "def lowerText(sent):\n",
    "  return list(map(lambda word: word.lower(), sent))\n",
    "\n",
    "def removeNumbers(sent):\n",
    "  return list(filter(lambda word: re.match(r'\\d+', word) is None, sent))\n",
    "\n",
    "def removePunctuation(sent):\n",
    "  return list(filter(lambda word: re.match(r'[a-zA-Z]+', word) is not None, sent))\n",
    "\n",
    "def removeStopWords(sent):\n",
    "  stopWords = set(stopwords.words(\"english\"))\n",
    "  return list(filter(lambda word: word not in stopWords, sent))\n",
    "\n",
    "def lemmatizeWords(sent):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  return list(map(lambda word: lemmatizer.lemmatize(word), sent))\n",
    "\n",
    "def preprocessData(sents):\n",
    "  sentences = list(map(lambda sent: lowerText(sent), sents))\n",
    "  sentences = list(map(lambda sent: removeNumbers(sent), sentences))\n",
    "  sentences = list(map(lambda sent: removePunctuation(sent), sentences))\n",
    "  sentences = list(map(lambda sent: removeStopWords(sent), sentences))\n",
    "  sentences = list(map(lambda sent: lemmatizeWords(sent), sentences))\n",
    "  return sentences\n",
    "\n",
    "start = time()\n",
    "cleanText = preprocessData(text)\n",
    "stop = time()\n",
    "\n",
    "print(f'Temps pour effectuer le prétraitement : {round(stop - start, 2)} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtY9--WV3bls"
   },
   "source": [
    "Retrait des mots rares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qYlid0gAiWkD",
    "outputId": "445aa607-c37b-429a-e557-ae0b41b5f0d0"
   },
   "outputs": [],
   "source": [
    "def removeRareWords(sents, min):\n",
    "  words = sentsToWords(sents)\n",
    "  frequencies = Counter(words)\n",
    "  return list(map(lambda sent: list(filter(lambda word: frequencies[word] >= min, sent)), sents))\n",
    "\n",
    "start = time()\n",
    "data = removeRareWords(cleanText, 2)\n",
    "stop = time()\n",
    "\n",
    "print(f'Temps pour effectuer le retrait des mots rares : {round(stop - start, 2)} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EZAJZWn098oB",
    "outputId": "bb562bdb-7fab-470f-98a8-f0c0e39ef1f0"
   },
   "outputs": [],
   "source": [
    "dataWords = sentsToWords(data)\n",
    "dataFrequencies = Counter(dataWords)\n",
    "\n",
    "print('Après le prétraitrement et le retrait des mots rares :\\n')\n",
    "print(f'Nombre de mots : {len(dataWords)}')\n",
    "print(f'Taille du vocabulaire : {len(dataFrequencies)}')\n",
    "print(f\"Première phrase : {' '.join(data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXBL4fEa9-31"
   },
   "source": [
    "# Entraînement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dX2Vz7M-oMW"
   },
   "source": [
    "Entraînement d'un modèle Word2Vec Skip-gram avec numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-qO_FDj52g0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mapWords(sents):\n",
    "  word2index, index2word = {}, {}\n",
    "\n",
    "  words = sentsToWords(sents)\n",
    "\n",
    "  for index, word in enumerate(set(words)):\n",
    "      word2index[word] = index\n",
    "      index2word[index] = word\n",
    "\n",
    "  return word2index, index2word\n",
    "\n",
    "def generateSkipgrams(sents, word2index, windowSize):\n",
    "  firstWords, secondWords = [], []\n",
    "  \n",
    "  for sent in sents:\n",
    "    sentLength = len(sent)\n",
    "    for i in range(sentLength):\n",
    "      neighborIndexes = list(range(max(0, i - windowSize), i)) + list(range(i + 1, min(sentLength, i + windowSize + 1)))\n",
    "      for j in neighborIndexes:\n",
    "          firstWords.append(word2index[sent[i]])\n",
    "          secondWords.append(word2index[sent[j]])\n",
    "          \n",
    "  firstWords = np.array(firstWords)\n",
    "  firstWords = np.expand_dims(firstWords, axis=0)\n",
    "  secondWords = np.array(secondWords)\n",
    "  secondWords = np.expand_dims(secondWords, axis=0)\n",
    "          \n",
    "  return firstWords, secondWords\n",
    "\n",
    "def initializeParameters(vocabSize, embSize):\n",
    "  parameters = {}\n",
    "  parameters['wordEmbs'] = np.random.randn(vocabSize, embSize) * 0.01\n",
    "  parameters['denseMatrix'] = np.random.randn(vocabSize, embSize) * 0.01\n",
    "  \n",
    "  return parameters\n",
    "    \n",
    "def softmax(denseLayerOut):\n",
    "  return np.divide(np.exp(denseLayerOut), np.sum(np.exp(denseLayerOut), axis=0, keepdims=True) + 0.001)\n",
    "\n",
    "def forwardPropagation(indexes, parameters):\n",
    "  wordEmbs = parameters['wordEmbs']\n",
    "  wordVector = wordEmbs[indexes.flatten(), :].T\n",
    "  denseMatrix = parameters['denseMatrix']\n",
    "  denseLayerOut = np.dot(denseMatrix, wordVector)\n",
    "  softmaxOut = softmax(denseLayerOut)\n",
    "  \n",
    "  caches = {}\n",
    "  caches['indexes'] = indexes\n",
    "  caches['wordVector'] = wordVector\n",
    "  caches['denseMatrix'] = denseMatrix\n",
    "  caches['denseLayerOut'] = denseLayerOut\n",
    "  \n",
    "  return softmaxOut, caches\n",
    "\n",
    "def computeLoss(softmaxOut, trainDataLabel):\n",
    "  return -(1 / softmaxOut.shape[1]) * np.sum(np.log(softmaxOut[trainDataLabel.flatten(), np.arange(trainDataLabel.shape[1])] + 0.001))\n",
    "\n",
    "def computeGradients(trainDataLabel, softmaxOut, caches):\n",
    "  denseMatrix = caches['denseMatrix']\n",
    "  wordVector = caches['wordVector']\n",
    "\n",
    "  softmaxOut[trainDataLabel.flatten(), np.arange(trainDataLabel.shape[1])] -= 1.0\n",
    "\n",
    "  dLdOut = softmaxOut\n",
    "  dLdDense = (1 / wordVector.shape[1]) * np.dot(dLdOut, wordVector.T)\n",
    "  dLdWordVector = np.dot(denseMatrix.T, dLdOut)\n",
    "  \n",
    "  gradients = dict()\n",
    "  gradients['dLdOut'] = dLdOut\n",
    "  gradients['dLdDense'] = dLdDense\n",
    "  gradients['dLdWordVector'] = dLdWordVector\n",
    "  \n",
    "  return gradients\n",
    "\n",
    "def updateParameters(parameters, caches, gradients, learningRate):\n",
    "  parameters['wordEmbs'][caches['indexes'].flatten(), :] -= gradients['dLdWordVector'].T * learningRate\n",
    "  parameters['denseMatrix'] -= learningRate * gradients['dLdDense']\n",
    "\n",
    "def numpyModelTraining(firstWords, secondWords, vocabSize, embSize, learningRate, epochs, batchSize):\n",
    "  costs = []\n",
    "  parameters = initializeParameters(vocabSize, embSize)\n",
    "  \n",
    "  start = time()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "      cost = 0\n",
    "      batchIndexes = list(range(0, firstWords.shape[1], batchSize))\n",
    "      np.random.shuffle(batchIndexes)\n",
    "\n",
    "      for index in batchIndexes:\n",
    "          firstWordBatch = firstWords[:, index:index+batchSize]\n",
    "          secondWordBatch = secondWords[:, index:index+batchSize]\n",
    "          softmaxOut, caches = forwardPropagation(firstWordBatch, parameters)\n",
    "          loss = computeLoss(softmaxOut, secondWordBatch)\n",
    "          gradients = computeGradients(secondWordBatch, softmaxOut, caches)\n",
    "          updateParameters(parameters, caches, gradients, learningRate)\n",
    "          cost += np.squeeze(loss)\n",
    "          \n",
    "      costs.append(cost)\n",
    "\n",
    "      if epoch % (epochs // 100) == 0:\n",
    "          learningRate *= 0.98\n",
    "\n",
    "  stop = time()\n",
    "\n",
    "  print(f'Temps pour entraîner le modèle : {round((stop - start) / 60, 2)} min\\n')\n",
    "\n",
    "  plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "  plt.title('Fonction de coût de l\\'entraînement de Skip-gram avec numpy')        \n",
    "  plt.plot(np.arange(epochs), costs)\n",
    "  plt.xlabel('Nombre d\\'epochs')\n",
    "  plt.ylabel('Coût')\n",
    "\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "colab_type": "code",
    "id": "9DHfu9WCIsd2",
    "outputId": "beaaaf47-4aa8-48ab-96af-b05b9d63f335"
   },
   "outputs": [],
   "source": [
    "word2index, index2word = mapWords(data)\n",
    "firstWords, secondWords = generateSkipgrams(data, word2index, 3)\n",
    "\n",
    "vocabSize = len(index2word)\n",
    "\n",
    "numpyModel = numpyModelTraining(firstWords, secondWords, vocabSize, 100, 0.05, 1000, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a39-KDGO3Gf"
   },
   "source": [
    "Enregistrement des plongements lexicaux du modèle Word2Vec Skip-gram entraîné avec numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5br8bd9O7FC"
   },
   "outputs": [],
   "source": [
    "numpyVectors = {}\n",
    "\n",
    "for index in np.arange(vocabSize):\n",
    "  numpyVectors[index2word[index]] = numpyModel['wordEmbs'][index]\n",
    "\n",
    "del numpyModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yh1PJ8LL4woZ"
   },
   "source": [
    "Entraînement d'un modèle Word2Vec Skip-gram avec Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "colab_type": "code",
    "id": "pB29JHf_3il5",
    "outputId": "c8599524-67a4-48e8-ffe5-8848d6c7767f"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "costs = []\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "  def __init__(self):\n",
    "    self.epoch = 0\n",
    "    self.cumulativeCost = 0\n",
    "\n",
    "  def on_epoch_end(self, model):\n",
    "    totalCost = model.get_latest_training_loss()\n",
    "    epochCost = totalCost - self.cumulativeCost\n",
    "    self.cumulativeCost = totalCost\n",
    "    costs.append(epochCost)\n",
    "    self.epoch += 1\n",
    "\n",
    "nbCores = multiprocessing.cpu_count()\n",
    "\n",
    "gensimModel = Word2Vec(sg=1,\n",
    "                       hs=1,\n",
    "                       min_count=2,\n",
    "                       window=3,\n",
    "                       size=100,\n",
    "                       alpha=0.05, \n",
    "                       min_alpha=0.006,\n",
    "                       workers=nbCores-1,\n",
    "                       batch_words=128)\n",
    "\n",
    "start = time()\n",
    "gensimModel.build_vocab(data)\n",
    "stop = time()\n",
    "\n",
    "print(f'Temps pour construire le vocabulaire : {round(stop - start, 2)} sec')\n",
    "\n",
    "start = time()\n",
    "gensimModel.train(data,\n",
    "                  total_examples=gensimModel.corpus_count,\n",
    "                  epochs=1000,\n",
    "                  compute_loss=True,\n",
    "                  callbacks=[callback()])\n",
    "stop = time()\n",
    "\n",
    "print(f'Temps pour entraîner le modèle : {round((stop - start) / 60, 2)} min\\n')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.title('Fonction de coût de l\\'entraînement de Skip-gram avec gensim')        \n",
    "plt.plot(np.arange(1000), costs)\n",
    "plt.xlabel('Nombre d\\'epochs')\n",
    "plt.ylabel('Coût')\n",
    "\n",
    "gensimModel.wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfrhRIxzRscY"
   },
   "source": [
    "Enregistrement des plongements lexicaux du modèle Word2Vec Skip-gram entraîné avec Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4KudTePD_-F"
   },
   "outputs": [],
   "source": [
    "gensimVectors = gensimModel.wv\n",
    "del gensimModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvKQ0TTcWvPw"
   },
   "source": [
    "Téléchargement du modèle de référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IdFlxov3MbSO",
    "outputId": "4cade538-c0b7-4b35-c79f-06141fb0d47b"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "googleModel = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NyKt618mmGbd"
   },
   "source": [
    "Enregistrement des plongements lexicaux du modèle de référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gn4dU3Mzk0hJ"
   },
   "outputs": [],
   "source": [
    "googleVectors = googleModel.wv\n",
    "del googleModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwXAS-I7VlUP"
   },
   "source": [
    "# Évaluation et comparaison des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wrlesjz_4t-I"
   },
   "source": [
    "Capacité à trouver les 5 mots les plus similaires à 1 mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNxXzuJ95CuF"
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def getTopNMostSimilar(vectors, word, n):\n",
    "  emb = vectors[word]\n",
    "  embs = vectors.copy()\n",
    "  del[embs[word]]\n",
    "  topN = []\n",
    "\n",
    "  for index in range(n):\n",
    "    minDist = 2\n",
    "    minWord = None\n",
    "    for item in embs.items():\n",
    "      dist = spatial.distance.cosine(emb, item[1])\n",
    "      if dist < minDist:\n",
    "        minDist = dist\n",
    "        minWord = item[0]\n",
    "    del[embs[minWord]]\n",
    "    topN.append(minWord)\n",
    "  \n",
    "  return topN\n",
    "\n",
    "def printTopNMostSimilar(ref, numpyResults, gensimResults, googleResults):\n",
    "  print(\"Mots les plus semblables à {} avec numpy:\".format(ref))\n",
    "  for index, word in enumerate(numpyResults):\n",
    "    print('{}. {}'.format(index + 1, word))\n",
    "\n",
    "  print(\"\\nMots les plus semblables à {} avec gensim:\".format(ref))\n",
    "  for index, word in enumerate(gensimResults):\n",
    "    print('{}. {}'.format(index + 1, word[0]))\n",
    "\n",
    "  print(\"\\nMots les plus semblables à {} avec google:\".format(ref))\n",
    "  for index, word in enumerate(googleResults):\n",
    "    print('{}. {}'.format(index + 1, word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OEpD6_CW5BOp",
    "outputId": "c6508b36-8a46-4fda-eafd-ac553dedfd80"
   },
   "outputs": [],
   "source": [
    "words = ['president', 'organization', 'veteran']\n",
    "\n",
    "for word in words:\n",
    "  numpySimilar = getTopNMostSimilar(numpyVectors, word, 5)\n",
    "  gensimSimilar = gensimVectors.similar_by_word(word, topn=5)\n",
    "  googleSimilar = googleVectors.similar_by_word(word, topn=5)\n",
    "  printTopNMostSimilar(word, numpySimilar, gensimSimilar, googleSimilar)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i84sVrj35DQ3"
   },
   "source": [
    "Capacité à trouver le mot qui complète une relation entre deux mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Qw7nGl64ZHT"
   },
   "outputs": [],
   "source": [
    "def completeRelation(vectors, pos1, neg1, pos2, n):\n",
    "  ref = vectors[pos1] - vectors[neg1] + vectors[pos2]\n",
    "  embs = vectors.copy()\n",
    "  del[embs[pos1]]\n",
    "  del[embs[neg1]]\n",
    "  del[embs[pos2]]\n",
    "  topN = []\n",
    "\n",
    "  for index in range(n):\n",
    "    minDist = 2\n",
    "    minWord = None\n",
    "    for item in embs.items():\n",
    "      dist = spatial.distance.cosine(ref, item[1])\n",
    "      if dist < minDist:\n",
    "        minDist = dist\n",
    "        minWord = item[0]\n",
    "    del[embs[minWord]]\n",
    "    topN.append(minWord)\n",
    "\n",
    "  return topN\n",
    "\n",
    "def printRelations(pos1, neg1, pos2, numpyResults, gensimResults, googleResults):\n",
    "  print(\"Mots complètent le mieux la relation {}, {} -> {}, _ avec numpy :\".format(pos1, neg1, pos2))\n",
    "  for index, word in enumerate(numpyResults):\n",
    "    print('{}. {}'.format(index + 1, word))\n",
    "\n",
    "  print(\"\\nMots complètent le mieux la relation {}, {} -> {}, _ avec gensim :\".format(pos1, neg1, pos2))\n",
    "  for index, word in enumerate(gensimResults):\n",
    "    print('{}. {}'.format(index + 1, word[0]))\n",
    "\n",
    "  print(\"\\nMots complètent le mieux la relation {}, {} -> {}, _ avec google :\".format(pos1, neg1, pos2))\n",
    "  for index, word in enumerate(googleResults):\n",
    "    print('{}. {}'.format(index + 1, word[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QoyijBnM5QMK",
    "outputId": "de29e36d-cdd0-4703-cf5c-167d86cd859b"
   },
   "outputs": [],
   "source": [
    "relations = [('country', 'president', 'city'),\n",
    "             ('georgia', 'state', 'atlanta' ),\n",
    "             ('one', 'two', 'three'),\n",
    "             ('hospital', 'doctor', 'school'),\n",
    "             ('year', 'month', 'week'),\n",
    "             ('supermarket', 'store', 'university'),\n",
    "             ('september', 'month', 'thursday')]\n",
    "\n",
    "for relation in relations:\n",
    "  numpyRelation = completeRelation(numpyVectors, relation[1], relation[0], relation[2], 3)\n",
    "  gensimRelation = gensimVectors.most_similar(positive=[relation[1], relation[2]], negative=[relation[0]], topn=3)\n",
    "  googleRelation = googleVectors.most_similar(positive=[relation[1], relation[2]], negative=[relation[0]], topn=3)\n",
    "  printRelations(relation[0], relation[1], relation[2], numpyRelation, gensimRelation, googleRelation)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0DVRKbhr5UPS"
   },
   "source": [
    "Capacité à séparer les mots les plus similaires des mots les plus différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvedBFVKWmbv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def getTopNLeastSimilar(vectors, word, n):\n",
    "  emb = vectors[word]\n",
    "  embs = vectors.copy()\n",
    "  del[embs[word]]\n",
    "  topN = []\n",
    "\n",
    "  for index in range(n):\n",
    "    maxDist = 0\n",
    "    maxWord = None\n",
    "    for item in embs.items():\n",
    "      dist = spatial.distance.cosine(emb, item[1])\n",
    "      if dist > maxDist:\n",
    "        maxDist = dist\n",
    "        maxWord = item[0]\n",
    "    del[embs[maxWord]]\n",
    "    topN.append(maxWord)\n",
    "  \n",
    "  return topN\n",
    "\n",
    "def tsneScatterPlot(model, vectors, ref, similar, different):\n",
    "  # Initialisation des arrays\n",
    "  embs = np.empty((0, len(vectors[ref])), dtype='f')\n",
    "  words = [ref]\n",
    "  colors = ['blue']\n",
    "\n",
    "  # Ajout du mot de référence\n",
    "  emb = vectors[ref]\n",
    "  embs = np.append(embs, emb.reshape(1, len(emb)), axis=0)\n",
    "\n",
    "  # Ajout des mots similaires\n",
    "  for word in similar:\n",
    "    emb = vectors[word]\n",
    "    words.append(word)\n",
    "    colors.append('green')\n",
    "    embs = np.append(embs, emb.reshape(1, len(emb)), axis=0)\n",
    "\n",
    "  # Ajout des mots différents\n",
    "  for word in different:\n",
    "    emb = vectors[word]\n",
    "    words.append(word)\n",
    "    colors.append('red')\n",
    "    embs = np.append(embs, emb.reshape(1, len(emb)), axis=0)\n",
    "\n",
    "  # Réduction de la dimension avec PCA\n",
    "  reducedEmbs = PCA().fit_transform(embs)\n",
    "\n",
    "  # Trouver les coordonnées en 2 dimensions avec t-SNE\n",
    "  np.set_printoptions(suppress=True)\n",
    "\n",
    "  coordinates = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reducedEmbs)\n",
    "\n",
    "  # Mise en forme pour l'affichage\n",
    "  df = pd.DataFrame({'x': [x for x in coordinates[:, 0]],\n",
    "                     'y': [y for y in coordinates[:, 1]],\n",
    "                     'words': words,\n",
    "                     'color': colors})\n",
    "\n",
    "  fig, _ = plt.subplots()\n",
    "  fig.set_size_inches(9, 9)\n",
    "\n",
    "  # Graphique de base\n",
    "  p1 = sns.regplot(data=df,\n",
    "                   x=\"x\",\n",
    "                   y=\"y\",\n",
    "                   fit_reg=False,\n",
    "                   marker=\"o\",\n",
    "                   scatter_kws={'s': 40,\n",
    "                                'facecolors': df['color']\n",
    "                               }\n",
    "                  )\n",
    "\n",
    "  # Annotations sur le graphique\n",
    "  for line in range(0, df.shape[0]):\n",
    "    p1.text(df[\"x\"][line],\n",
    "            df['y'][line],\n",
    "            '  ' + df[\"words\"][line].title(),\n",
    "            horizontalalignment='left',\n",
    "            verticalalignment='bottom', size='medium',\n",
    "            color=df['color'][line],\n",
    "            weight='normal'\n",
    "           ).set_size(15)\n",
    "\n",
    "  plt.xlim(coordinates[:, 0].min()-50, coordinates[:, 0].max()+50)\n",
    "  plt.ylim(coordinates[:, 1].min()-50, coordinates[:, 1].max()+50)\n",
    "\n",
    "  plt.title('Visualisation t-SNE pour {} avec {}'.format(ref.title(), model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LxeT8X2nEfCi",
    "outputId": "35ecca54-ac49-47da-8c12-e57a0f180745"
   },
   "outputs": [],
   "source": [
    "numpySim = getTopNMostSimilar(numpyVectors, 'school', 10)\n",
    "gensimSim = gensimVectors.similar_by_word('school', topn=10)\n",
    "googleSim = googleVectors.similar_by_word('school', topn=10)\n",
    "\n",
    "numpyDiff = getTopNLeastSimilar(numpyVectors, 'school', 10)\n",
    "gensimDiff = gensimVectors.most_similar(negative=['school'], topn=10)\n",
    "googleDiff = googleVectors.most_similar(negative=['school'], topn=10)\n",
    "\n",
    "tsneScatterPlot('numpy', numpyVectors, 'school', numpySim, numpyDiff)\n",
    "tsneScatterPlot('gensim', gensimVectors, 'school', [i[0] for i in gensimSim], [i[0] for i in gensimDiff])\n",
    "tsneScatterPlot('google', googleVectors, 'school', [i[0] for i in googleSim], [i[0] for i in googleDiff])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "inf8225_projet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
